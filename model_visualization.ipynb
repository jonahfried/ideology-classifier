{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting a classification model using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# import time\n",
    "\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "\n",
    "from data_processing import LeftRightProcessor, convert_examples_to_features\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"./output\"\n",
    "data_dir = \"./data/data\"\n",
    "device = torch.device(\"cpu\")\n",
    "num_labels = 2\n",
    "max_seq_length = 128\n",
    "bert_model = \"bert-base-uncased\"\n",
    "cased = False\n",
    "seed = 42\n",
    "num_test_examples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(features):\n",
    "    \"\"\" Converts a set of features to a TensorDataset \"\"\"\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    all_case_ids = torch.tensor([f.case_id for f in features], dtype=torch.long)\n",
    "    return TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_case_ids)\n",
    "\n",
    "def get_eval_dataloader(eval_features, eval_batch_size=8):\n",
    "    \"\"\" parses test examples and prepares them into a DataLoader \"\"\"\n",
    "    eval_dataset = get_dataset(eval_features)\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    return DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_tensors(text, max_seq_length, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) > max_seq_length - 2 :\n",
    "         tokens = tokens[:(max_seq_length - 2)]\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    input_id = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    input_mask = torch.tensor([input_mask], dtype=torch.long).to(device)\n",
    "    segment_id = torch.tensor([segment_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    return input_id, input_mask, segment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_text(text_lst):\n",
    "    probs = []\n",
    "    model.eval()\n",
    "    for text in text_lst:\n",
    "        input_ids, input_mask, segment_ids = text_to_tensors(text, max_seq_length, tokenizer)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            probs.append([float(n) for n in logits[0]])\n",
    "    return np.array(probs)\n",
    "\n",
    "# model_text([test_examples[0].text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    high_confidence_cutoff = 4\n",
    "    low_confidence_cutoff = 1\n",
    "    def __init__(self, predicted, real, logits, case_id):\n",
    "        self.predicted = predicted\n",
    "        self.real = real\n",
    "        self.logits = logits\n",
    "        self.case_id = case_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"predicted: {}, real: {}, logits: {}, case_id: {}\".format(self.predicted, self.real, self.logits, self.case_id)\n",
    "    \n",
    "    def get_confidence(self):\n",
    "        \"\"\" Returns the magnitude of the range of logits \"\"\"\n",
    "        return abs(self.logits[1] - self.logits[0])\n",
    "    \n",
    "    def get_confidence_group(self):\n",
    "        \"\"\" Returns:\n",
    "        -1 : Confident Wrong\n",
    "        0 : Unsure\n",
    "        1 : Confident Correct\n",
    "        None : No clear distinction\n",
    "        \"\"\"\n",
    "        confidence = self.get_confidence()\n",
    "        if confidence > self.high_confidence_cutoff:\n",
    "            return 1 if self.is_correct() else -1\n",
    "        elif confidence < self.low_confidence_cutoff:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def is_correct(self):\n",
    "        return self.predicted == self.real\n",
    "    \n",
    "    @classmethod\n",
    "    def set_high_confidence_cutoff(cls, cutoff):\n",
    "        cls.high_confidence_cutoff = cutoff\n",
    "        \n",
    "    @classmethod\n",
    "    def set_low_confidence_cutoff(cls, cutoff):\n",
    "        cls.low_confidence_cutoff = cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_individually(test_features, model, verbosity=0):\n",
    "    predictions = []\n",
    "    dl = get_eval_dataloader(test_features, eval_batch_size=1)\n",
    "    model.eval()\n",
    "    for input_ids, input_mask, segment_ids,label_ids, case_ids in dl: #tqdm_notebook(dl, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        case_ids = case_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        predictions.append(Prediction(outputs[0], label_ids[0], logits[0], case_ids[0]))\n",
    "        if verbosity:\n",
    "            print(\"predicted:\", outputs, \"real_label:\", label_ids, \"probs:\", logits, \"id:\", case_ids)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Model For Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2019 13:40:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /n/home12/jdcclark/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=(not cased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processor = LeftRightProcessor(\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    data_dir=data_dir,\n",
    "    seed=seed,\n",
    "    train_batch_size=1, #shouldn't be used\n",
    "    eval_batch_size=1\n",
    ")\n",
    "test_examples = processor.get_examples(num_test_examples, \"test.csv\")\n",
    "# test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = convert_examples_to_features(test_examples, processor.get_labels(), max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model From output_dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/22/2019 13:40:50 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ./output\n",
      "03/22/2019 13:40:50 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.99 s, sys: 264 ms, total: 3.25 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir, num_labels=num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to predict each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "predictions = predict_individually(test_features, model, verbosity=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_wrong = []\n",
    "unsure = []\n",
    "confident_correct = []\n",
    "for prediction in predictions:\n",
    "    category =  prediction.get_confidence_group()\n",
    "    if category == 1:\n",
    "        confident_correct.append(prediction)\n",
    "    if category == 0:\n",
    "        unsure.append(prediction)\n",
    "    if category == -1:\n",
    "        confident_wrong.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sort(key=Prediction.get_confidence, reverse=True)\n",
    "incorrect_predictions = list(filter(lambda x: not Prediction.is_correct(x), predictions))\n",
    "incorrect_predictions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME Interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import json\n",
    "\n",
    "class_names = [\"left\", \"right\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "exp_tokenizer = lambda doc: re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/features.json\") as f:\n",
    "    vec_features = [json.loads(line) for line in f]\n",
    "id_to_text = {feature[\"linex_index\"]:feature[\"text\"] for feature in vec_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names, split_expression=exp_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = id_to_text[615791547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "exp = explainer.explain_instance(ex, model_text, num_features=10, num_samples=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
